<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image Pair</title>
	<meta property="og:image" content="./resources/fusion_teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image Pair" />
	<meta property="og:description" content="A learning-based framework for recovering video sequence from a pair of blurred-coded images." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image Pair</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Anupama S</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://asprasan.github.io">Prasan Shedligeri</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Abhishek Pal</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://www.ee.iitm.ac.in/kmitra/">Kaushik Mitra</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2010.10052'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/asprasan/codedblurred'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<!-- <table align=center width=850px> -->
			<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-spacing:0;}
.tg td{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;
  padding:5px 5px;word-break:normal;}
.tg th{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-8jgo{border-color:#ffffff;text-align:center;vertical-align:center}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:center}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-8jgo"></th>
    <th class="tg-8jgo">Fully-Exposed</th>
    <th class="tg-8jgo">Coded</th>
    <th class="tg-c3ow" colspan="2">Fully exposed - coded (Ours)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Input</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/figure0/cvpr18/seq_01_blurred.png" width="125" height="125"></td>
    <td class="tg-8jgo"><img src="./resources/figure0/1b/seq_01_bucket01.png" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/c2b/seq_01_blurred.png" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/c2b/seq_01_bucket01.png" width="125" height="125"></td>
  </tr>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Output</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/figure0/cvpr18/cvpr18_01.gif" width="125" height="125"></td>
    <td class="tg-8jgo"><img src="./resources/figure0/1b/p2c2_01.gif" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/c2b/p2c2_01.gif" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/gt/gt_01.gif" width="125" height="125"></td>
  </tr>
  <tr>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">24.35 dB / 0.968</td>
    <td class="tg-c3ow">28.80 dB / 0.950</td>
    <td class="tg-c3ow"><span style="font-weight:bold">34.07dB / 0.982</span></td>
    <td class="tg-c3ow">Ground Truth</td>
  </tr>
  <tr>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">Fully-exposed</td>
    <td class="tg-c3ow">Coded</td>
    <td class="tg-c3ow" colspan="2">Fully exposed - coded (Ours)</td>
  </tr>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Input</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/figure0/cvpr18/seq_06_blurred.png" width="125" height="125"></td>
    <td class="tg-8jgo"><img src="./resources/figure0/1b/seq_06_bucket01.png" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/c2b/seq_06_blurred.png" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/c2b/seq_06_bucket01.png" width="125" height="125"></td>
  </tr>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Output</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/figure0/cvpr18/cvpr18_06.gif" width="125" height="125"></td>
    <td class="tg-8jgo"><img src="./resources/figure0/1b/p2c2_06.gif" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/c2b/p2c2_06.gif" width="125" height="125"></td>
    <td class="tg-c3ow"><img src="./resources/figure0/gt/gt_06.gif" width="125" height="125"></td>
  </tr>
  <tr>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow">20.13dB / 0.862</td>
    <td class="tg-c3ow">33.75dB / 0.968</td>
    <td class="tg-c3ow"><span style="font-weight:bold">35.59dB / 0.978</span></td>
    <td class="tg-c3ow">Ground Truth</td>
  </tr>
</tbody>
</table>
			<!-- <tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/fusion_teaser.png"/>
					</center>
				</td>
			</tr> -->
		<!-- </table> -->
		<!-- <table align=center width=850px>
			<tr>
				<td>
					This was a template originally made for <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a>. The code can be found in this <a href="https://github.com/richzhang/webpage-template">repository</a>.
				</td>
			</tr>
		</table> -->
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td align=justify>
				Learning-based methods have enabled the recovery of a video sequence from a single motion-blurred image or a single coded exposure image. Recovering video from a single motion-blurred image is a very ill-posed problem and the recovered video usually has many artifacts. In addition to this, the direction of motion is lost and it results in motion ambiguity. However, it has the advantage of fully preserving the information in the static parts of the scene. The traditional coded exposure framework is better-posed but it only samples a fraction of the space-time volume, which is at best 50% of the space-time volume. Here, we propose to use the complementary information present in the fully-exposed (blurred) image along with the coded exposure image to recover a high fidelity video without any motion ambiguity. Our framework consists of a shared encoder followed by an attention module to selectively combine the spatial information from the fully-exposed image with the temporal information from the coded image, which is then super-resolved to recover a non-ambiguous high-quality video. The input to our algorithm is a fully-exposed and coded image pair. Such an acquisition system already exists in the form of a Coded-two-bucket (C2B) camera. We demonstrate that our proposed deep learning approach using blurred-coded image pair produces much better results than those from just a blurred image or just a coded image. 
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/RfXMlGg1kOI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href='https://docs.google.com/presentation/d/1VxjBlD70bW-hMiSQlJLm6akre9zi0_Ob6bt6pZO5EdE/edit?usp=sharing'>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	<hr>

	<center><h1>Key takeaways</h1></center>

	<table align=center width=800px>
		<center>
			<tr>
				<td>
					<li>
						We propose a learning-based framework for recovering video sequence from a pair of blurred-coded images.
					</li>
					<li>
						The framework consists of an attenion-based feature fusion module which learns to fuse features from blurred image and the coded image.
					</li>
					<li>
						We show qualitatively and quantitatively that higher-fidelity video can be recovered when using both blurred-coded images than from either blurred or coded image alone.
					</li>
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Code</h1></center>

	<!-- <table align=center width=640px>
		<center>
			<tr>
				<td>

				</td>
			</tr>
		</center>
	</table> -->
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=justify>
					Our proposed framework first extracts a low-spatial resolution video sequence from blurred and coded images separately.
					A shared encoder network is used to exract features from the two video sequences.
					An attention-based fusion module based on a similarity measure is used to fuse features from the blurred and coded images.
					A deep U-Net is then used to extract the full spatial and temporal resolution video sequence from the fused features.
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/asprasan/codedblurred'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt"> Anupama S., P. Shedligeri, Abhishek Pal, Kaushik Mitra<br>
				<b> Video Reconstruction by Spatio-temporal Fusion of Blurred-Coded Image Pair</b><br>
				In ICPR, 2020.<br>
				(hosted on <a href="https://arxiv.org/abs/2010.10052">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=300px>
		<tr>
			<td align=center><span style="font-size:14pt">
				<a href="https://drive.google.com/file/d/1u99_tjrFW56qvVXm46CmA-zBOvI1-56o/view?usp=sharing">[Supplementary Material]</a></td>
			<td align=center ><span style="font-size:14pt">
				<a href="./resources/bibtex.txt">[Bibtex]</a></td>
		</tr>
	</table>

	<hr>
	<br>

	<center><h1>Related Publications</h1></center>

	<table align=center width=900px>
		<center>
			<tr>
				<td>
					<li><b>Prasan Shedligeri</b>, Anupama S & Kaushik Mitra. (2021) A Unified Framework for Compressive Video Recovery from Coded ExposureTechniques. Accepted at <em>IEEE/CVF Winter Conference on Applications of Computer Vision,</em> doi to be assigned 
            	<a href="https://arxiv.org/abs/2011.05532">[Preprint]</a>
            	<a href="https://docs.google.com/presentation/d/1TqyRWTtNqIssMJ_nnHTTNLNbdP_oMqGtBlgvnaNTcYI/edit?usp=sharing">[Slides]</a>
            	<a href="https://drive.google.com/file/d/1GGpgzTGXnE1XzONJOv81iK23TVIfkAvB/view?usp=sharing">[Supplementary]</a>
            	<a href="https://github.com/asprasan/unified_framework">[Code]</a>
            <a href="https://asprasan.github.io/unified_framework/">[Webpage]</a></li>

	           		<li><b>Prasan Shedligeri</b>, Anupama S & Kaushik Mitra. (2021) CodedRecon: Video reconstruction for coded exposure imaging techniques. Accepted at <em>Elsevier Journal of Software Impacts,</em> https://doi.org/10.1016/j.simpa.2021.100064
	            	<a href="https://www.sciencedirect.com/science/article/pii/S2665963821000129">[Paper]</a>
	            	<a href="https://github.com/asprasan/unified_framework">[Code]</a></li>
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					The authors would like to thank <a href="https://sreyas-mohan.github.io/">Sreyas Mohan</a> and <a href="https://subeeshvasu.github.io/">Subeesh Vasu</a> for their helpful discussions.
					<br>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

